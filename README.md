# TranscriberApp

TranscriberApp es una herramienta modular diseÃ±ada para:

- **Transcribir audios mediante Whisper** con aceleraciÃ³n GPU en Jetson
- **Procesar textos directamente** con anÃ¡lisis avanzado
- **Generar resÃºmenes inteligentes** usando Gemini (Google Generative AI)
- **Extraer tareas de reuniones** (modo refinamiento)
- **Crear resÃºmenes tÃ©cnicos, ejecutivos o en bullet points**
- **Guardar resultados en formato Markdown**
- **Ejecutar en contenedores Docker** con soporte CUDA completo

---

## ğŸš€ CaracterÃ­sticas principales

- **TranscripciÃ³n automÃ¡tica** de archivos `.mp3` con Whisper en GPU
- **Procesamiento directo** de archivos `.txt`
- **Modos de anÃ¡lisis**: tÃ©cnico, refinamiento, ejecutivo, bullet, default
- **Salida estructurada** en `outputs/`
- **Transcripciones guardadas** en `transcripts/`
- **Arquitectura modular y extensible**
- **Soporte completo para NVIDIA Jetson** (Orin Nano, Xavier, etc.)
- **DockerizaciÃ³n completa** con soporte CUDA
- **Wheels personalizados** para PyTorch CUDA en JetPack 6.x

---

## ğŸ–¥ï¸ Compatibilidad

### **Entornos soportados:**
- âœ… **NVIDIA Jetson** (Orin Nano, Xavier, AGX Orin) con JetPack 6.x
- âœ… **Ubuntu 22.04+** con NVIDIA GPU
- âœ… **Docker con soporte NVIDIA GPU**
- âœ… **Entornos virtuales Python 3.10**

### **Requisitos especÃ­ficos para Jetson:**
- JetPack 6.0 o superior
- CUDA 12.2+
- Python 3.10
- 8GB+ RAM recomendado

---

## ğŸ“¦ Stack tecnolÃ³gico

### **Backend:**
- **Whisper** (OpenAI) - TranscripciÃ³n de audio
- **PyTorch 2.3.0 + CUDA 12.4** - AceleraciÃ³n GPU
- **FastAPI** - API web
- **Google Gemini API** - AnÃ¡lisis y resumen de texto
- **ONNX Runtime GPU** - OptimizaciÃ³n inferencia

### **Infraestructura:**
- **Docker** con runtime NVIDIA
- **Docker Compose** para orquestaciÃ³n
- **Wheels personalizados** para compatibilidad Jetson

### **Dependencias principales:**
```txt
google-generativeai      # Cliente oficial para modelos Gemini
torch>=2.3.0             # PyTorch con CUDA para Jetson
whisper                  # Motor de transcripciÃ³n de audio
fastapi                  # Framework web asÃ­ncrono
uvicorn                  # Servidor ASGI
```

---

## ğŸ—ï¸ Arquitectura del proyecto

```
TranscriberApp/
â”‚
â”œâ”€â”€ audios/                     # Archivos .mp3 de entrada
â”œâ”€â”€ transcripts/                # Transcripciones generadas (.txt)
â”œâ”€â”€ outputs/                    # Resultados finales (.md)
â”œâ”€â”€ wheels/                     # Wheels personalizados para Jetson
â”‚   â”œâ”€â”€ torch_cuda_jetpack-2.3.0-py3-none-any.whl    # PyTorch CUDA personalizado
â”‚   â”œâ”€â”€ torchaudio-2.3.0+952ea74-cp310-cp310-linux_aarch64.whl
â”‚   â”œâ”€â”€ torchvision-0.18.0a0+6043bc2-cp310-cp310-linux_aarch64.whl
â”‚   â”œâ”€â”€ openai_whisper-20250625-py3-none-any.whl
â”‚   â””â”€â”€ onnxruntime_gpu-1.19.0-cp310-cp310-linux_aarch64.whl
â”‚
â”œâ”€â”€ transcriber_app/
â”‚   â”œâ”€â”€ main.py                 # Punto de entrada principal
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ audio_receiver.py   # Carga de audio
â”‚   â”‚   â”œâ”€â”€ audio_downloader.py # Descarga de audio desde URL
â”‚   â”‚   â”œâ”€â”€ transcriber.py      # Whisper con CUDA
â”‚   â”‚   â”œâ”€â”€ gemini_client.py    # Cliente Gemini
â”‚   â”‚   â”œâ”€â”€ summarizer.py       # LÃ³gica de resumen
â”‚   â”‚   â”œâ”€â”€ output_formatter.py # Guardado de resultados
â”‚   â”‚   â”œâ”€â”€ prompt_factory.py   # Prompts por modo
â”‚   â”‚   â””â”€â”€ logging/            # ConfiguraciÃ³n de logs
â”‚   â”œâ”€â”€ runner/
â”‚   â”‚   â””â”€â”€ orchestrator.py     # OrquestaciÃ³n del pipeline
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ web_app.py          # AplicaciÃ³n FastAPI
â”‚       â”œâ”€â”€ api/                # Endpoints REST
â”‚       â””â”€â”€ static/             # Interfaz web
â”‚
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n Docker
â”œâ”€â”€ Dockerfile                  # Imagen Docker optimizada para Jetson
â”œâ”€â”€ requirements_clean.txt      # Dependencias Python
â”œâ”€â”€ requirements.txt           # Dependencias completas (incluye wheels)
â”œâ”€â”€ .env                       # Variables de entorno
â””â”€â”€ README.md
```

---

## âš™ï¸ InstalaciÃ³n

### **OpciÃ³n 1: Entorno virtual (desarrollo)**

```bash
# 1. Clonar repositorio
git clone <repositorio>
cd TranscriberApp

# 2. Crear entorno virtual
python3 -m venv venv_transcriber
source venv_transcriber/bin/activate

# 3. Instalar dependencias
pip install -r requirements_clean.txt

# 4. Instalar wheels CUDA personalizados (Jetson)
pip install wheels/torch_cuda_jetpack-2.3.0-py3-none-any.whl
pip install wheels/torchaudio-2.3.0+952ea74-cp310-cp310-linux_aarch64.whl
pip install wheels/torchvision-0.18.0a0+6043bc2-cp310-cp310-linux_aarch64.whl
pip install wheels/openai_whisper-20250625-py3-none-any.whl
pip install wheels/onnxruntime_gpu-1.19.0-cp310-cp310-linux_aarch64.whl

# 5. Configurar API Key
echo "GEMINI_API_KEY=TU_API_KEY_AQUI" > .env
```

### **OpciÃ³n 2: Docker (producciÃ³n)**

```bash
# 1. Construir imagen (con soporte CUDA)
docker build -t transcriberapp:golden .

# 2. Verificar CUDA en contenedor
docker run --rm --gpus all transcriberapp:golden \
  python3 -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')"

# 3. Ejecutar con Docker Compose
docker-compose up -d
```

### **OpciÃ³n 3: Docker Compose (recomendada)**

```yaml
# docker-compose.yml
version: "3.9"

services:
  transcriberapp:
    build: .
    image: transcriberapp:golden
    container_name: transcriberapp
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "8000:8000"
    volumes:
      - ./audios:/app/audios
      - ./outputs:/app/outputs
      - ./transcripts:/app/transcripts
    env_file:
      - .env
```

```bash
# Ejecutar
docker-compose up -d
```

---

## ğŸ¯ Modos disponibles

| Modo | DescripciÃ³n | Uso tÃ­pico |
|------|-------------|------------|
| `default` | Resumen simple y general | Reuniones informales |
| `tecnico` | Resumen tÃ©cnico avanzado | Sprint planning, revisiones tÃ©cnicas |
| `refinamiento` | Extrae tareas, subtareas, decisiones y backlog | Refinement sessions |
| `ejecutivo` | Resumen ejecutivo conciso (5-8 lÃ­neas) | Reportes a direcciÃ³n |
| `bullet` | Resumen en puntos clave | Notas rÃ¡pidas, seguimiento |

---

## ğŸš€ EjecuciÃ³n

### **1. CLI (modo desarrollo)**

```bash
# Activar entorno
source venv_transcriber/bin/activate

# Transcribir audio
python -m transcriber_app.main audio ejemplo1 tecnico

# Procesar texto existente
python -m transcriber_app.main texto ejemplo1 refinamiento

# Descargar audio desde URL
python transcriber_app/modules/audio_downloader.py "https://youtube.com/watch?v=..."
```

### **2. Web API (modo producciÃ³n)**

```bash
# Iniciar servidor web
uvicorn transcriber_app.web.web_app:app --host 0.0.0.0 --port 8000

# O usando Docker
docker-compose up -d
```

Acceder a: `http://localhost:8000`

### **3. Docker CLI**

```bash
# Transcribir audio
docker run --rm --gpus all \
  -v $(pwd)/audios:/app/audios \
  -v $(pwd)/outputs:/app/outputs \
  transcriberapp:golden \
  python3 -m transcriber_app.main audio ejemplo1 tecnico

# Verificar CUDA
docker run --rm --gpus all transcriberapp:golden \
  python3 -c "import torch; print(f'Torch CUDA: {torch.cuda.is_available()}')"
```

---

## ğŸ“ Estructura de archivos generados

### **Entrada:**
```
audios/
â””â”€â”€ reunion1.mp3
```

### **Salida:**
```
transcripts/
â””â”€â”€ reunion1.txt          # TranscripciÃ³n completa

outputs/
â””â”€â”€ reunion1_tecnico.md   # Resumen analizado
```

### **Formato del archivo .md:**
```markdown
# Resumen TÃ©cnico - reunion1

## ğŸ“ Resumen
[Resumen generado por Gemini...]

## ğŸ”§ Puntos tÃ©cnicos clave
- [Punto 1...]
- [Punto 2...]

## ğŸ¯ PrÃ³ximos pasos
- [AcciÃ³n 1...]
- [AcciÃ³n 2...]

---
*Generado por TranscriberApp con Whisper + Gemini*
```

---

## ğŸ”§ ConfiguraciÃ³n avanzada

### **Variables de entorno (.env):**
```bash
GEMINI_API_KEY=tu_api_key_aqui
CUDA_VISIBLE_DEVICES=0
MODEL_SIZE=base              # Whisper model: tiny, base, small, medium
TARGET_LANG=es               # Idioma objetivo
LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
```

### **ConfiguraciÃ³n de Whisper:**
```python
# En transcriber_app/modules/transcriber.py
MODEL_SIZE = "base"          # Balance entre velocidad y precisiÃ³n
DEVICE = "cuda"              # Usar GPU
COMPUTE_TYPE = "float16"     # PrecisiÃ³n mixta para Jetson
```

### **ConfiguraciÃ³n de Gemini:**
```python
# En transcriber_app/modules/gemini_client.py
MODEL_NAME = "gemini-2.5-flash-lite"  # Modelo Gemini
TEMPERATURE = 0.7
MAX_TOKENS = 2048
```

---

## ğŸ› SoluciÃ³n de problemas

### **Problema: CUDA no disponible en Docker**
```bash
# Verificar que Docker tiene acceso a GPU
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi

# Reconstruir con soporte NVIDIA
docker build --no-cache -t transcriberapp:golden .
```

### **Problema: PyTorch sin CUDA en Jetson**
```bash
# Usar wheels personalizados incluidos
pip install wheels/torch_cuda_jetpack-2.3.0-py3-none-any.whl

# Verificar instalaciÃ³n
python -c "import torch; print(torch.cuda.is_available())"
```

### **Problema: Memoria insuficiente en Jetson**
```bash
# Usar modelo Whisper mÃ¡s pequeÃ±o
export MODEL_SIZE=tiny

# Reducir batch size
export WHISPER_BATCH_SIZE=1
```

### **Problema: API Gemini no responde**
```bash
# Verificar API key
echo $GEMINI_API_KEY

# Probar conexiÃ³n
python -c "import google.generativeai as genai; genai.configure(api_key='TU_KEY'); print('OK')"
```

---

## ğŸ“Š Rendimiento en Jetson Orin Nano

| Componente | Tiempo (30min audio) | Memoria GPU |
|------------|----------------------|-------------|
| Whisper (base) | ~2-3 minutos | ~2GB |
| Gemini (flash-lite) | ~5-10 segundos | <1GB |
| Total pipeline | ~3-4 minutos | ~3GB |

**Optimizaciones aplicadas:**
- PyTorch compilado para CUDA 12.4
- Whisper con soporte FP16
- Modelo Gemini optimizado para baja latencia
- Cache de modelos en memoria

---

## ğŸ”„ Flujo de trabajo tÃ­pico

1. **Grabar reuniÃ³n** â†’ `reunion_sprint.mp3`
2. **Subir audio** a `audios/`
3. **Ejecutar transcripciÃ³n**:
   ```bash
   python -m transcriber_app.main audio reunion_sprint refinamiento
   ```
4. **Revisar resultados** en `outputs/reunion_sprint_refinamiento.md`
5. **Exportar a Jira/Linear** (manual o script)

---

## ğŸ“ˆ Roadmap

### **PrÃ³ximas caracterÃ­sticas:**
- [ ] ExportaciÃ³n automÃ¡tica a Jira/Linear
- [ ] Modo "acta de reuniÃ³n" con asistencia
- [ ] Resumen para email automÃ¡tico
- [ ] Dashboard web con historial
- [ ] Soporte multi-idioma automÃ¡tico
- [ ] Cache inteligente de transcripciones

### **Mejoras tÃ©cnicas:**
- [ ] Whisper large-v3 con optimizaciones
- [ ] Streaming en tiempo real
- [ ] DiarizaciÃ³n (identificaciÃ³n de hablantes)
- [ ] CompresiÃ³n de audio inteligente

---

## ğŸ›¡ï¸ Notas importantes

### **Seguridad:**
- Las API keys se almacenan en `.env` (no commitear)
- Las transcripciones se guardan localmente
- Conexiones SSL para APIs externas

### **Limitaciones:**
- Audio mÃ¡ximo recomendado: 60 minutos
- Requiere conexiÃ³n a Internet para Gemini
- Jetson requiere JetPack 6.x para CUDA 12.4

### **Backup de wheels CUDA:**
```bash
# Los wheels personalizados son Ãºnicos
cp wheels/torch_cuda_jetpack-2.3.0-py3-none-any.whl ~/backups/
# Guardar en mÃºltiples ubicaciones
```

---

## ğŸ¤ Contribuciones

1. Fork el repositorio
2. Crear rama de feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

---

## ğŸ“„ Licencia

Distribuido bajo la licencia MIT. Ver `LICENSE` para mÃ¡s informaciÃ³n.

---

## âœ¨ Agradecimientos

- **OpenAI** por Whisper
- **Google** por Gemini API
- **NVIDIA** por JetPack y soporte Jetson
- **FastAPI** por el framework web
- **Todos los contribuidores** de cÃ³digo abierto

---

## ğŸ“ Soporte

Para soporte, abrir un issue en GitHub o contactar al mantenedor.

**Â¡Happy transcribing! ğŸ™ï¸â†’ğŸ“**

---

*Ãšltima actualizaciÃ³n: Enero 2025*  
*VersiÃ³n: 2.0.0 (Gold Edition)*  
*Optimizado para NVIDIA Jetson con CUDA*